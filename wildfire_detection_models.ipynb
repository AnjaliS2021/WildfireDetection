{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnjaliS2021/WildfireDetection/blob/main/wildfire_detection_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one\n",
        "# or more contributor license agreements. See the NOTICE file\n",
        "# distributed with this work for additional information\n",
        "# regarding copyright ownership. The ASF licenses this file\n",
        "# to you under the Apache License, Version 2.0 (the\n",
        "# \"License\"); you may not use this file except in compliance\n",
        "# with the License. You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing,\n",
        "# software distributed under the License is distributed on an\n",
        "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "# KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations\n",
        "# under the License."
      ],
      "metadata": {
        "id": "xCRnNNaiFNFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Owner: Anjali Singh\n",
        "\n",
        "Data resources\n",
        "- wildfire_detection_dataset.ipynb\n",
        "\n",
        "Modeling resources\n",
        "- Earth Engine https://developers.google.com/earth-engine/guides/machine-learning\n",
        "- U-Net https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
        "- Segformer https://keras.io/examples/vision/segformer/"
      ],
      "metadata": {
        "id": "0gyRAGiOzSsD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5HmAQyxanlb"
      },
      "source": [
        "#üõ∞ Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIdChZMncckO"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovXi8tffW9y0"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "# Keep the imports in sorted order.\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from tensorflow import keras\n",
        "\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "print(f'py version: {sys.version}')\n",
        "print(f'tf version: {tf.__version__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbekcFDMuf_g"
      },
      "outputs": [],
      "source": [
        "#@title Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "wildfire_drive_path = '/content/drive/MyDrive/wildfire_detection'\n",
        "print(os.listdir(wildfire_drive_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwC3a7nVVauo"
      },
      "source": [
        "# üß† U-Net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxuk4WXk2NmO"
      },
      "outputs": [],
      "source": [
        "#@title üìñ Read Dataset\n",
        "class UnetDatasetBuilder:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.image_size = None  # Will be updated from dataset.\n",
        "    self.num_bands = 3\n",
        "    self.min_fire_mask = 7\n",
        "    self.max_fire_mask = 9\n",
        "    self.fire_masks = self.get_fire_masks()\n",
        "    self.num_classes = self.get_num_classes()\n",
        "    self.max_frp = 2400\n",
        "    self.batch_size = 16\n",
        "    self.build_paths()\n",
        "    self.build_dataset()\n",
        "\n",
        "  def __str__(self):\n",
        "    return '\\n'.join([\n",
        "      f'fire_masks: {self.fire_masks}',\n",
        "      f'image size: {self.image_size}',\n",
        "      f'batch size: {self.batch_size}',\n",
        "      f'train dataset: {self.train_dataset.element_spec}',\n",
        "      f'test dataset: {self.test_dataset.element_spec}',\n",
        "      f'keras.backend.image_data_format: {tf.keras.backend.image_data_format()}',\n",
        "    ])\n",
        "\n",
        "  def build_paths(self):\n",
        "    self.base_directory = wildfire_drive_path\n",
        "    self.dataset_directory = 'dataset'\n",
        "    self.dataset_path = os.path.join(\n",
        "      self.base_directory, self.dataset_directory)\n",
        "    self.train_path = os.path.join(self.dataset_path, 'train')\n",
        "    self.test_path = os.path.join(self.dataset_path, 'test')\n",
        "\n",
        "  def get_fire_masks(self):\n",
        "    fire_masks = [0]\n",
        "    fire_masks.extend(range(self.min_fire_mask, self.max_fire_mask+1))\n",
        "    return fire_masks\n",
        "\n",
        "  def get_num_classes(self):\n",
        "    num_classes = max(self.fire_masks) + 1\n",
        "    return num_classes\n",
        "\n",
        "  def build_dataset(self):\n",
        "    self.train_dataset = self.load_dataset(self.train_path)\n",
        "    self.test_dataset = self.load_dataset(self.test_path)\n",
        "\n",
        "  def load_dataset(self, dataset_path):\n",
        "    # Use tf.data.TFRecordDataset to read the TFRecord files. It gets bytes for\n",
        "    # each element. The read_example converts that into image and label tensors.\n",
        "    filenames = tf.data.Dataset.list_files(\n",
        "        f'{dataset_path}/*/part-*.tfrecord.gz')\n",
        "    print(f'Loading num files: {len(list(filenames.as_numpy_iterator()))}')\n",
        "    dataset = tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "    dataset = dataset.map(self.read_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    self.set_image_size(dataset)\n",
        "    dataset = dataset.map(self.update_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(self.batch_size).shuffle(\n",
        "        buffer_size=16, reshuffle_each_iteration=True).prefetch(\n",
        "            buffer_size=16)\n",
        "    return dataset\n",
        "\n",
        "  def read_example(self, serialized):\n",
        "    features_dict = {\n",
        "      'inputs': tf.io.FixedLenFeature([], tf.string),\n",
        "      'labels': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    example_tf = tf.io.parse_single_example(serialized, features_dict)\n",
        "    inputs = tf.io.parse_tensor(example_tf['inputs'], tf.int32)\n",
        "    labels = tf.io.parse_tensor(example_tf['labels'], tf.int64)\n",
        "    # TensorFlow can't infer the shapes, so we set them explicitly.\n",
        "    inputs.set_shape([None, None, self.num_bands])\n",
        "    labels.set_shape([None, None, 1])\n",
        "    example = {'inputs': inputs, 'labels': labels}\n",
        "    return example\n",
        "\n",
        "  def update_example(self, example):\n",
        "    inputs = example['inputs']\n",
        "    labels = example['labels']\n",
        "    inputs, labels = self.normalize(inputs, labels)\n",
        "    # Resize as expected by the U-Net.\n",
        "    image_size = self.image_size\n",
        "    inputs = tf.image.resize(inputs, (image_size, image_size),\n",
        "                             method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    labels = tf.image.resize(labels, (image_size, image_size),\n",
        "                             method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    example.update(self.format(inputs, labels))\n",
        "    inputs = example['inputs']\n",
        "    labels = example['labels']\n",
        "    return inputs, labels\n",
        "\n",
        "  def normalize(self, input_image, input_mask):\n",
        "    input_image = tf.clip_by_value(input_image, 0, self.max_frp)\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    scale = 255.0 / self.max_frp\n",
        "    input_image *= scale\n",
        "    input_mask = self.normalize_label(input_mask)\n",
        "    return input_image, input_mask\n",
        "\n",
        "  def normalize_label(self, input_mask):\n",
        "    input_mask = tf.cast(input_mask, tf.int32)\n",
        "    return input_mask\n",
        "\n",
        "  def format(self, input_image, input_mask):\n",
        "    return {'inputs': input_image, 'labels': input_mask}\n",
        "\n",
        "  def set_image_size(self, dataset):\n",
        "    self.image_size = 160 # This size is required by Unet.\n",
        "\n",
        "  def debug_dataset(self):\n",
        "    print('train dataset')\n",
        "    dataset = self.train_dataset\n",
        "    dataset = dataset.unbatch()\n",
        "    for inputs, labels in dataset.take(1):\n",
        "      print(f'inputs : {inputs.dtype.name} {inputs.shape}')\n",
        "      print(inputs)\n",
        "      print(f'labels : {labels.dtype.name} {labels.shape}')\n",
        "      print(labels)\n",
        "\n",
        "unet_dataset_builder = UnetDatasetBuilder()\n",
        "print(unet_dataset_builder)\n",
        "unet_dataset_builder.debug_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-W4S0V0jIp6"
      },
      "source": [
        "## üèõ Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn7_casz05II"
      },
      "outputs": [],
      "source": [
        "#@title Create U-Net\n",
        "class UnetModelBuilder:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dataset_builder = unet_dataset_builder\n",
        "    self.image_size = self.dataset_builder.image_size\n",
        "    self.num_bands = self.dataset_builder.num_bands\n",
        "    self.num_classes = self.dataset_builder.num_classes\n",
        "    self.build_paths()\n",
        "    self.build_model()\n",
        "    self.compile_model()\n",
        "    self.build_checkpoint()\n",
        "\n",
        "  def __str__(self):\n",
        "    return '\\n'.join([\n",
        "      f'num_bands: {self.num_bands}',\n",
        "      f'num_classes: {self.num_classes}'\n",
        "    ])\n",
        "\n",
        "  def build_paths(self):\n",
        "    self.base_directory = wildfire_drive_path\n",
        "    self.model_directory = 'unet_model'\n",
        "    self.model_path = os.path.join(self.base_directory, self.model_directory)\n",
        "    self.checkpoint_directory = 'unet_checkpoint'\n",
        "    self.checkpoint_path = os.path.join(\n",
        "        self.base_directory, self.checkpoint_directory, 'unet')\n",
        "\n",
        "  def build_model(self):\n",
        "    inputs = keras.Input(shape=(self.image_size, self.image_size, self.num_bands))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "        x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "        x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = keras.layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "        x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "        x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = keras.layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = keras.layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = keras.layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = keras.layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = keras.layers.Conv2D(self.num_classes, 3, activation=\"softmax\", padding=\"same\")(\n",
        "        x\n",
        "    )\n",
        "\n",
        "    # Define the model\n",
        "    self.model = keras.Model(inputs, outputs, name=\"UNet\")\n",
        "    self.model.summary()\n",
        "\n",
        "  def compile_model(self):\n",
        "    unet8s_optimizer = keras.optimizers.Adam()\n",
        "    unet8s_loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "    # Maintain mIOU and Pixel-wise Accuracy as metrics\n",
        "    self.model.compile(\n",
        "        optimizer=unet8s_optimizer,\n",
        "        loss=unet8s_loss,\n",
        "        metrics=[\n",
        "            keras.metrics.MeanIoU(num_classes=self.num_classes, sparse_y_pred=False),\n",
        "            keras.metrics.SparseCategoricalAccuracy(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "  def build_checkpoint(self):\n",
        "    self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        self.checkpoint_path, save_weights_only=True)\n",
        "    checkpoint_directory = os.path.dirname(self.checkpoint_path)\n",
        "    latest_checkpoint_path = tf.train.latest_checkpoint(checkpoint_directory)\n",
        "    if latest_checkpoint_path:\n",
        "      print(f'Load checkpoint: {latest_checkpoint_path}')\n",
        "      self.model.load_weights(latest_checkpoint_path)\n",
        "\n",
        "  def train_and_save_model(self, epochs=1):\n",
        "    self.build_checkpoint()\n",
        "    self.train_model(epochs)\n",
        "    self.save_model()\n",
        "\n",
        "  def train_model(self, epochs=1):\n",
        "    train_dataset = self.dataset_builder.train_dataset\n",
        "    test_dataset = self.dataset_builder.test_dataset\n",
        "    self.history = self.model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=test_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[self.checkpoint_callback]\n",
        "    )\n",
        "\n",
        "  def save_model(self):\n",
        "    self.model.save(self.model_path)\n",
        "\n",
        "  def load_model(self):\n",
        "    self.model = tf.keras.models.load_model(self.model_path)\n",
        "\n",
        "  def run_predict(self, inputs, labels=None):\n",
        "    if labels is not None:\n",
        "      labels = labels[0]\n",
        "      if tf.is_tensor(labels):\n",
        "        labels = labels.numpy().astype(np.uint8)\n",
        "    probabilities = self.model.predict(inputs)\n",
        "    probabilities = probabilities[0]\n",
        "    if tf.is_tensor(probabilities):\n",
        "      probabilities = probabilities.numpy()\n",
        "    predictions = probabilities.argmax(axis=-1).astype(np.uint8)\n",
        "    if labels is not None:\n",
        "      predictions.resize(labels.shape)\n",
        "    return predictions, labels\n",
        "\n",
        "unet_model_builder = UnetModelBuilder()\n",
        "print(unet_model_builder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an0Cj7yrVFKd"
      },
      "outputs": [],
      "source": [
        "#@title Train the model.\n",
        "#unet_model_builder.train_and_save_model(epochs=1)\n",
        "print(os.listdir(unet_model_builder.model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvHxFXxph8Nk"
      },
      "source": [
        "## üîÆ Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2jujIGmpBN7"
      },
      "outputs": [],
      "source": [
        "#@title Analyze Model\n",
        "class UnetModelAnalyzer:\n",
        "\n",
        "  def __init__(self, dataset_builder=unet_dataset_builder,\n",
        "               model_builder=unet_model_builder):\n",
        "    self.dataset_builder = dataset_builder\n",
        "    self.model_builder = model_builder\n",
        "\n",
        "  def run_analysis(self):\n",
        "    all_examples = self.build_dataset()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    for inputs, labels in all_examples:\n",
        "      predictions, labels = self.run_predict(inputs, labels)\n",
        "      all_predictions.append(predictions)\n",
        "      all_labels.append(labels)\n",
        "    self.show_matrix(all_predictions, all_labels)\n",
        "\n",
        "  def build_dataset(self, max_examples=5):\n",
        "    all_examples = []\n",
        "    dataset = self.dataset_builder.test_dataset.unbatch().batch(1)\n",
        "    for inputs, labels in dataset:\n",
        "      if max_examples <= 0:\n",
        "        break\n",
        "      max_examples -= 1\n",
        "      all_examples.append([inputs, labels])\n",
        "    return all_examples\n",
        "\n",
        "  def run_predict(self, inputs, labels):\n",
        "    predictions, labels = self.model_builder.run_predict(inputs, labels)\n",
        "    predictions = predictions.flatten()\n",
        "    labels = labels.flatten()\n",
        "    return predictions, labels\n",
        "\n",
        "  def show_matrix(self, all_predictions, all_labels):\n",
        "    y_true = np.concatenate(all_labels, axis=None)\n",
        "    y_pred = np.concatenate(all_predictions, axis=None)\n",
        "    fire_masks = self.dataset_builder.fire_masks\n",
        "\n",
        "    cfr = classification_report(y_true, y_pred, labels=fire_masks, zero_division=0)\n",
        "    print(cfr)\n",
        "    cfm = confusion_matrix(y_true, y_pred, labels=fire_masks)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n",
        "                                  display_labels=fire_masks)\n",
        "    cfm_norm = confusion_matrix(y_true, y_pred, labels=fire_masks, normalize='true')\n",
        "    disp_norm = ConfusionMatrixDisplay(confusion_matrix=cfm_norm, display_labels=fire_masks)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "    disp.plot(ax=ax1)\n",
        "    disp_norm.plot(ax=ax2)\n",
        "    plt.show()\n",
        "\n",
        "unet_model_analyzer = UnetModelAnalyzer()\n",
        "unet_model_analyzer.run_analysis()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejPs2MxJ0SvY"
      },
      "source": [
        "# ü§ó Vision Transformer\n",
        "Details at SegFormer and Hugging Face Transformers\n",
        "* https://keras.io/examples/vision/segformer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiE_aP7Bzs0D"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from transformers import TFSegformerForSemanticSegmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVIHlfRic9bK"
      },
      "outputs": [],
      "source": [
        "#@title üìñ Read Dataset\n",
        "class SegFormerDatasetBuilder(UnetDatasetBuilder):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SegFormerDatasetBuilder, self).__init__()\n",
        "    self.build_paths()\n",
        "    self.build_dataset()\n",
        "\n",
        "  def get_num_classes(self):\n",
        "    num_classes = 0\n",
        "    for mask in self.fire_masks:\n",
        "      if mask > 0:\n",
        "        num_classes += 1\n",
        "    return num_classes\n",
        "\n",
        "  def set_image_size(self, dataset):\n",
        "    self.image_size = 512  # This size is required by SegFormer.\n",
        "\n",
        "  def update_example(self, example):\n",
        "    inputs = example['inputs']\n",
        "    labels = example['labels']\n",
        "    # Resize as expected by the SegFormer.\n",
        "    image_size = self.image_size\n",
        "    inputs = tf.image.resize(inputs, (image_size, image_size),\n",
        "                             method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    labels = tf.image.resize(labels, (image_size, image_size),\n",
        "                             method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    # SegFormer expects normalized pixel values.\n",
        "    inputs, labels = self.normalize(inputs, labels)\n",
        "    example = self.format(inputs, labels)\n",
        "    return example\n",
        "\n",
        "  def normalize(self, input_image, input_mask):\n",
        "    input_image = tf.clip_by_value(input_image, 0, self.max_frp)\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    input_image /= self.max_frp  # range: 0 to 1.\n",
        "    # These values are specified by the SegFormer.\n",
        "    pretrained_mean = tf.constant([0.485, 0.456, 0.406])\n",
        "    pretrained_std = tf.constant([0.229, 0.224, 0.225])\n",
        "    input_image = (input_image - pretrained_mean) / tf.maximum(\n",
        "        pretrained_std, tf.keras.backend.epsilon())\n",
        "    input_mask = tf.cast(input_mask, tf.int32)\n",
        "    return input_image, input_mask\n",
        "\n",
        "  def format(self, input_image, input_mask):\n",
        "    # Transpose the images such that they are in 'channels_first' format. This\n",
        "    # is to make them compatible with the SegFormer model from Hugging Face\n",
        "    # Transformers.\n",
        "    input_image = tf.transpose(input_image, (2, 0, 1))\n",
        "    return {'pixel_values': input_image, 'labels': tf.squeeze(input_mask)}\n",
        "\n",
        "  def debug_dataset(self):\n",
        "    print('train dataset')\n",
        "    dataset = self.train_dataset\n",
        "    dataset = dataset.unbatch()\n",
        "    for example in dataset.take(1):\n",
        "      print(example)\n",
        "\n",
        "segformer_dataset_builder = SegFormerDatasetBuilder()\n",
        "print(segformer_dataset_builder)\n",
        "segformer_dataset_builder.debug_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgslm1cg8Cr"
      },
      "source": [
        "## üèõ Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nq4SaNySblj"
      },
      "outputs": [],
      "source": [
        "#@title Create SegFormer\n",
        "class SegFormerModelBuilder:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dataset_builder = segformer_dataset_builder\n",
        "    self.fire_masks = self.dataset_builder.fire_masks\n",
        "    self.image_size = self.dataset_builder.image_size\n",
        "    self.checkpoint = \"nvidia/mit-b0\"\n",
        "    self.build_paths()\n",
        "    self.build_model()\n",
        "    self.compile_model()\n",
        "    self.build_checkpoint()\n",
        "\n",
        "  def __str__(self):\n",
        "    return '\\n'.join([\n",
        "      f'SegFormer checkpoint: {self.checkpoint}',\n",
        "      f'labels: {self.id2label}',\n",
        "      f'fire_masks: {self.fire_masks}'\n",
        "    ])\n",
        "\n",
        "  def build_paths(self):\n",
        "    self.base_directory = wildfire_drive_path\n",
        "    self.model_directory = 'segformer_model'\n",
        "    self.model_path = os.path.join(self.base_directory, self.model_directory)\n",
        "    self.checkpoint_directory = 'segformer_checkpoint'\n",
        "    self.checkpoint_path = os.path.join(\n",
        "        self.base_directory, self.checkpoint_directory, 'segformer')\n",
        "\n",
        "  def build_model(self):\n",
        "    self.build_id2label()\n",
        "    self.build_label2id()\n",
        "    self.num_labels = len(self.id2label)\n",
        "    self.model = TFSegformerForSemanticSegmentation.from_pretrained(\n",
        "        self.checkpoint,\n",
        "        num_labels=self.num_labels,\n",
        "        id2label=self.id2label,\n",
        "        label2id=self.label2id,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    self.model.summary()\n",
        "\n",
        "  def build_id2label(self):\n",
        "    max_fire_mask = max(self.fire_masks)\n",
        "    id2label = {}\n",
        "    for i in range(max_fire_mask + 1):\n",
        "      id2label[i] = str(i)\n",
        "    self.id2label = id2label\n",
        "\n",
        "  def build_label2id(self):\n",
        "    max_fire_mask = max(self.fire_masks)\n",
        "    label2id = {}\n",
        "    for i in range(max_fire_mask + 1):\n",
        "      label2id[str(i)] = i\n",
        "    self.label2id = label2id\n",
        "\n",
        "  def compile_model(self):\n",
        "    self.model.compile(optimizer='adam')\n",
        "\n",
        "  def build_checkpoint(self):\n",
        "    self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        self.checkpoint_path, save_weights_only=True)\n",
        "    checkpoint_directory = os.path.dirname(self.checkpoint_path)\n",
        "    latest_checkpoint_path = tf.train.latest_checkpoint(checkpoint_directory)\n",
        "    if latest_checkpoint_path:\n",
        "      print(f'Load checkpoint: {latest_checkpoint_path}')\n",
        "      self.model.load_weights(latest_checkpoint_path)\n",
        "\n",
        "  def train_and_save_model(self, epochs=1):\n",
        "    self.build_checkpoint()\n",
        "    self.train_model(epochs)\n",
        "    self.save_model()\n",
        "\n",
        "  def train_model(self, epochs=1):\n",
        "    train_dataset = self.dataset_builder.train_dataset\n",
        "    test_dataset = self.dataset_builder.test_dataset\n",
        "    self.history = self.model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=test_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[self.checkpoint_callback]\n",
        "    )\n",
        "\n",
        "  def save_model(self):\n",
        "    self.model.save(self.model_path)\n",
        "\n",
        "  def load_model(self):\n",
        "    self.model = tf.keras.models.load_model(self.model_path)\n",
        "\n",
        "  def run_predict(self, inputs, labels=None):\n",
        "    image_size = self.image_size\n",
        "    probabilities = self.model.predict(inputs).logits\n",
        "    predictions = tf.math.argmax(probabilities, axis=1)\n",
        "    predictions = predictions[0]\n",
        "    predictions = tf.expand_dims(predictions, -1)\n",
        "    predictions = tf.image.resize(predictions, (image_size, image_size),\n",
        "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    predictions = tf.squeeze(predictions)\n",
        "    predictions = predictions.numpy().astype(np.uint8)\n",
        "    if labels is not None:\n",
        "      labels = labels[0]\n",
        "      if tf.is_tensor(labels):\n",
        "        labels = labels.numpy().astype(np.uint8)\n",
        "    return predictions, labels\n",
        "\n",
        "\n",
        "segformer_model_builder = SegFormerModelBuilder()\n",
        "print(segformer_model_builder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts4qom5AOCb2"
      },
      "outputs": [],
      "source": [
        "#@title Train the model\n",
        "#segformer_model_builder.train_and_save_model(epochs=1)\n",
        "#print(os.listdir(segformer_model_builder.model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-nzRd_iUn9U"
      },
      "outputs": [],
      "source": [
        "#@title Analyze Model\n",
        "class SegFormerModelAnalyzer(UnetModelAnalyzer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SegFormerModelAnalyzer, self).__init__(\n",
        "        dataset_builder=segformer_dataset_builder,\n",
        "        model_builder=segformer_model_builder)\n",
        "\n",
        "  def build_dataset(self, max_examples=5):\n",
        "    all_examples = []\n",
        "    dataset = self.dataset_builder.test_dataset.unbatch().batch(1)\n",
        "    for example in dataset:\n",
        "      if max_examples <= 0:\n",
        "        break\n",
        "      max_examples -= 1\n",
        "      inputs, labels = example[\"pixel_values\"], example[\"labels\"]\n",
        "      all_examples.append([inputs, labels])\n",
        "    return all_examples\n",
        "\n",
        "segformer_model_analyzer = SegFormerModelAnalyzer()\n",
        "segformer_model_analyzer.run_analysis()"
      ]
    }
  ]
}